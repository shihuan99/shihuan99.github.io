## 消息堆积典型问题

> 消息堆积问题绝大部分都是客户端业务逻辑、客户端环境引起的。

**消息堆积问题一定一定要先确认消费处理是否变慢（下游依赖有阻塞）**，确定了消费没问题再根据典型问题排查：



### 1、消费组重平衡异常

#### 1.1、max.poll.interval.ms配置过大

现象：消费组Rebalance时间长（正常一次Rebalance耗时1分钟）

如果max.poll.interval.ms参数设置过大的值，可能出现一个消费者卡住导致长时间无法完成rebalance。只要这个消费者在max.poll.interval.ms时间内没处理完消息，所有消费者都得等它调用poll()发送JOIN_GROUP。

#### 1.2、消费者频繁退出导致重平衡反复执行

现象：消费组频繁出现重平衡，并且重平衡能结束，持续短暂时间后再次出现重平衡。

消费组Stable后能消费到消息，消息处理逻辑有问题导致进程退出（遇到过OOM导致进程异常的）。

#### 1.3、消费组内成员太多导致重平衡失败

现象：Kafka的消费组Coordinator节点处理请求特别慢，requestQueueTime长达几秒。定位发现有个消费组有一万多个成员，导致Coordinator的IO线程卡在JoinGroupRequestHandler中，JoinGroup逻辑需要获取group锁。

解决方案：不同Topic的业务不要使用同个消费组，避免成员过多在rebalance时导致Coordinator负载高。

#### 1.4、消费处理慢引发重平衡

max.poll.interval.ms默认为5分钟，消费者通过poll()拉取到一批消息后，如果处理这些消息时间超过5分钟就会触发LeaveGroup导致消费组重平衡。

#### 1.5、消费者连接池存在问题导致重平衡无法结束

Spark Stream框架维护了一个消费者数组，用来管理消费者的创建和轮询poll()，这些消费者属于同个消费组，每个消费者订阅一个不同的Topic。

由于这些消费者由线程池ForkJoinPool管理，默认是16个线程，当消费者数量超过这个值时，就会导致有些消费者没能及时的JoinGroup，重平衡反复执行（300s超时消费者会退出引发新一轮的重平衡）。因此在使用线程池管理消费者时，特别注意线程数量和消费者数量的关系，确保消费者能定期轮询。



### 2、消费位点出现重置

现象为消息堆积突增，生产流量没增加，且有消费到历史消息。这类问题客户端会有明显的Resetting offset的日志，应该从客户端日志入手排查，一般是因为消费者有段时间没消费，然后消息保留时间到期删除，再用原来的位点去消费会报错。

#### 2.1、消息保留时间过期，导致OffsetOutOfRange异常触发位点重置

实际场景一般是隔了很长一段时间才消费（中间卡住了或者故意定时消费），老化时间设置的比较短（比如一个小时），在一个小时后再消费的话就可能出现消息被删了，导致消费者报错触发位点重置。

#### 2.2、磁盘不够导致消息被删（云厂商有磁盘清理策略）

磁盘不够导致消息被删，一般是不会删正在写入的消息，建议消费者需要保持消费，不要堆积太多。

#### 2.3、Flink的位点初始化策略没配好

Flink如果开启checkpoint会优先从checkpoint获取之前消费位点，如果没有配置checkpoint，则可能从Kafka获取位点或者重置到指定时间，需要关注Flink的位点初始化配置，建议使用checkpoint。



### 3、消费请求变慢

#### 3.1、sendTime高

服务端Kafka往消费者socket channel推送数据耗时长，可能是中间网络瓶颈（比如公网带宽流控）、消费者内存不够(buffer.memory、socket.receive.buffer放不下了，在消息比较大的时候可能出现这种情况）。

#### 3.2、localTime高

服务端读磁盘耗时高，iostat检查磁盘负载。

#### 3.3、requestQueueTime高

服务端IO线程压力大，检查请求TPS，哪些请求占比多，存在哪些慢请求阻塞了IO线程